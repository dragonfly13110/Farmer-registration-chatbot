import os
import re
import shutil

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≤‡∏Å LangChain
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_core.documents import Document

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å ---
# ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå Markdown ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å
KB_MARKDOWN_PATH = "data/knowledge_base.md"
# ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Vector Store ‡πÉ‡∏´‡πâ‡∏™‡∏∑‡πà‡∏≠‡∏ñ‡∏∂‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á
VECTORSTORE_PATH = "vectorstore_smart_chunking_v2" 
# Embedding Model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á (Chunking Strategies) ---

def chunk_by_headers(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ Markdown (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢, ‡∏Å‡∏é‡πÄ‡∏Å‡∏ì‡∏ë‡πå, ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)
    ‡πÉ‡∏ä‡πâ #, ##, ###, #### ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ï‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô
    """
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on,
        strip_headers=False  # ‡∏Ñ‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô context
    )
    docs = markdown_splitter.split_text(text)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏¢‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk ‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô
    final_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    for doc in docs:
        if len(doc.page_content) > 800:
            sub_docs = text_splitter.create_documents([doc.page_content], metadatas=[doc.metadata])
            final_docs.extend(sub_docs)
        else:
            final_docs.append(doc)
    return final_docs

def chunk_definitions(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏®‡∏±‡∏û‡∏ó‡πå (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏Ñ‡∏∑‡∏≠ 1 chunk)
    ‡πÉ‡∏ä‡πâ Regular Expression ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô "#### **1. ‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£**")
    """
    # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Regex ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Markdown ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤ (**) ‡πÑ‡∏î‡πâ
    pattern = re.compile(r"(####\s*(\*\*)*\d+\..*?(?=\n####\s*(\*\*)*\d+\.|\Z))", re.DOTALL)
    
    # findall ‡∏Å‡∏±‡∏ö capturing group ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á tuple, ‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà match ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (index 0)
    matches = pattern.findall(text)
    definitions = [match[0] for match in matches]
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Document
    docs = [Document(page_content=d.strip()) for d in definitions if d.strip()]
    return docs

def chunk_table_like_data(text: str, chunk_prefix: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£/‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏≠ 1 chunk)
    ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£ split by newline ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° Prefix ‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° context
    """
    lines = text.strip().split('\n')
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ '*'
    items = [line.strip() for line in lines if line.strip().startswith('*')]

    docs = []
    for item in items:
        # ‡∏ô‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ * ‡πÅ‡∏•‡∏∞ ** ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        clean_item = item.replace('*', '', 1).replace('**', '').strip()
        page_content = f"{chunk_prefix}: {clean_item}"
        docs.append(Document(page_content=page_content))
    return docs

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store"""
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡πÅ‡∏ö‡∏ö Smart Chunking...")
    
    if not os.path.exists(KB_MARKDOWN_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {KB_MARKDOWN_PATH}")
        return

    # --- 1. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .md ---
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å: {KB_MARKDOWN_PATH}")
    loader = TextLoader(KB_MARKDOWN_PATH, encoding="utf-8")
    full_text = loader.load()[0].page_content
    
    # ‡πÉ‡∏ä‡πâ re.split ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô ---[SECTION:NAME]---
    section_delimiter_pattern = r'---\[SECTION:(.*?)\]---'
    parts = re.split(section_delimiter_pattern, full_text)

    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å re.split
    # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô [‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô‡πÅ‡∏£‡∏Å, ‡∏ä‡∏∑‡πà‡∏≠section1, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤section1, ‡∏ä‡∏∑‡πà‡∏≠section2, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤section2, ...]
    if len(parts) > 1:
        # ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å (index 0) ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà ‡∏ä‡∏∑‡πà‡∏≠ ‡∏Å‡∏±‡∏ö ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        section_names = parts[1::2]      # ‡πÄ‡∏≠‡∏≤ index 1, 3, 5, ...
        section_contents = parts[2::2]   # ‡πÄ‡∏≠‡∏≤ index 2, 4, 6, ...
        section_map = {name.strip(): content.strip() for name, content in zip(section_names, section_contents)}
    else:
        section_map = {}

    if not section_map:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô '---[SECTION:...]--' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå! ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Recursive ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ó‡∏ô")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        all_documents = text_splitter.create_documents([full_text])
    else:
        # --- 2. ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Chunking ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô ---
        print(f"‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ {len(section_map)} ‡∏™‡πà‡∏ß‡∏ô! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô...")
        all_documents = []
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠ Section ‡πÅ‡∏•‡∏∞‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
        strategy_map = {
            "DEFINITIONS": ("üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'DEFINITIONS' ‡πÅ‡∏ö‡∏ö chunk-per-definition...", chunk_definitions, {}),
            "RULES": ("üìú ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'RULES' ‡πÅ‡∏ö‡∏ö chunk-by-header...", chunk_by_headers, {}),
            "HOW_TO_GUIDE": ("üë£ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'HOW_TO_GUIDE' ‡πÅ‡∏ö‡∏ö chunk-by-header...", chunk_by_headers, {}),
            "MAINTENANCE": ("‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'MAINTENANCE' ‡πÅ‡∏ö‡∏ö chunk-by-header...", chunk_by_headers, {}),
            "TIMELINES": ("‚è∞ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'TIMELINES' ‡πÅ‡∏ö‡∏ö chunk-by-header...", chunk_by_headers, {}),
            "PLANTING_DENSITY": ("üå≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'PLANTING_DENSITY' ‡πÅ‡∏ö‡∏ö chunk-per-item...", chunk_table_like_data, {"chunk_prefix": "‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏£‡πà"})
        }

        for name, content in section_map.items():
            if name in strategy_map:
                message, chunk_func, kwargs = strategy_map[name]
                print(f"  - {message}")
                docs = chunk_func(content, **kwargs)
                for doc in docs:
                    doc.metadata["source"] = name.lower() # ‡πÉ‡∏™‡πà metadata ‡∏ö‡∏≠‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤
                all_documents.extend(docs)
                print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")
            else:
                print(f"  - ‚ùì ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{name}', ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Recursive ‡πÅ‡∏ó‡∏ô...")
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
                docs = text_splitter.create_documents([content])
                all_documents.extend(docs)


    if not all_documents:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏î‡πÜ ‡πÑ‡∏î‡πâ! ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
        return

    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ---
    print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_documents)} ‡∏ä‡∏¥‡πâ‡∏ô...")
    
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Embedding Model: {EMBEDDING_MODEL}")
    # ‡πÉ‡∏ä‡πâ GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL, 
        model_kwargs={'device': 'cuda' if 'CUDA_VISIBLE_DEVICES' in os.environ else 'cpu'}
    )
    
    if os.path.exists(VECTORSTORE_PATH):
        print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö Vector Store ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà '{VECTORSTORE_PATH}'...")
        shutil.rmtree(VECTORSTORE_PATH)

    db = FAISS.from_documents(all_documents, embeddings)
    db.save_local(VECTORSTORE_PATH)
    print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store (Smart Chunking v2) ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {VECTORSTORE_PATH}")

if __name__ == "__main__":
    main()