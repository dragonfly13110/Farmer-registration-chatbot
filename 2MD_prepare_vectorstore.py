import os
import re
import shutil

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≤‡∏Å LangChain
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_core.documents import Document

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å ---
# ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå Markdown ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å
KB_MARKDOWN_PATH = "data/knowledge_base.md"
QNA_MARKDOWN_PATH = "data/Q&A.md" # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå Q&A.md
# ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Vector Store ‡πÉ‡∏´‡πâ‡∏™‡∏∑‡πà‡∏≠‡∏ñ‡∏∂‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á
VECTORSTORE_PATH = "vectorstore_smart_chunking_v2" 
# Embedding Model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á (Chunking Strategies) ---

def chunk_by_headers(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ Markdown (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢, ‡∏Å‡∏é‡πÄ‡∏Å‡∏ì‡∏ë‡πå, ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)
    ‡πÉ‡∏ä‡πâ #, ##, ###, #### ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ï‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô
    """
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on,
        strip_headers=False  # ‡∏Ñ‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô context
    )
    docs = markdown_splitter.split_text(text)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏¢‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk ‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô
    final_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    for doc in docs:
        if len(doc.page_content) > 800:
            sub_docs = text_splitter.create_documents([doc.page_content], metadatas=[doc.metadata])
            final_docs.extend(sub_docs)
        else:
            final_docs.append(doc)
    return final_docs

def chunk_definitions(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏®‡∏±‡∏û‡∏ó‡πå (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏Ñ‡∏∑‡∏≠ 1 chunk)
    ‡πÉ‡∏ä‡πâ Regular Expression ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô "#### **1. ‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£**")
    """
    # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Regex ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Markdown ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤ (**) ‡πÑ‡∏î‡πâ
    pattern = re.compile(r"(####\s*(\*\*)*\d+\..*?(?=\n####\s*(\*\*)*\d+\.|\Z))", re.DOTALL)
    
    # findall ‡∏Å‡∏±‡∏ö capturing group ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á tuple, ‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà match ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (index 0)
    matches = pattern.findall(text)
    definitions = [match[0] for match in matches]
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Document
    docs = [Document(page_content=d.strip()) for d in definitions if d.strip()]
    return docs

def chunk_table_like_data(text: str, chunk_prefix: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£/‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏≠ 1 chunk)
    ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£ split by newline ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° Prefix ‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° context
    """
    lines = text.strip().split('\n')
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ '*'
    items = [line.strip() for line in lines if line.strip().startswith('*')]

    docs = []
    for item in items:
        # ‡∏ô‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ * ‡πÅ‡∏•‡∏∞ ** ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        clean_item = item.replace('*', '', 1).replace('**', '').strip()
        page_content = f"{chunk_prefix}: {clean_item}"
        docs.append(Document(page_content=page_content))
    return docs

def parse_qna_markdown(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á Q&A ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Q&A.md
    ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô 1 chunk
    """
    qna_docs = []
    lines = text.split('\n')
    current_category = ""
    current_subcategory = ""
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # Extract main category (## ‡∏´‡∏°‡∏ß‡∏î)
        if line.startswith('## ‡∏´‡∏°‡∏ß‡∏î'):
            current_category = line.replace('## ‡∏´‡∏°‡∏ß‡∏î', '').strip()
            current_subcategory = "" # Reset subcategory
            i += 1
            continue
        # Extract subcategory (### ‡∏´‡∏°‡∏ß‡∏î)
        elif line.startswith('### ‡∏´‡∏°‡∏ß‡∏î'):
            current_subcategory = line.replace('### ‡∏´‡∏°‡∏ß‡∏î', '').strip()
            i += 1
            continue
        
        # Check for question pattern: 1.  **‡∏ñ‡∏≤‡∏°:** "..."
        q_match = re.match(r'^\d+\.\s+\*\*‡∏ñ‡∏≤‡∏°:\*\*\s*(.*)$', line)
        if q_match:
            question_text = q_match.group(1).strip()
            # Remove leading/trailing quotes if they exist
            if question_text.startswith('"') and question_text.endswith('"'):
                question_text = question_text[1:-1]
            
            # Look for answer in the next line(s)
            answer_lines = []
            j = i + 1
            while j < len(lines):
                next_line = lines[j].strip()
                if next_line.startswith('> **‡∏ï‡∏≠‡∏ö:**'):
                    answer_lines.append(next_line.replace('> **‡∏ï‡∏≠‡∏ö:**', '').strip())
                    j += 1
                    # Continue collecting blockquote lines if they are part of the same answer
                    while j < len(lines) and lines[j].strip().startswith('>'):
                        answer_lines.append(lines[j].strip().lstrip('>').strip())
                        j += 1
                    break # Found answer, break inner loop
                elif next_line.strip() == '---' or re.match(r'^\d+\.\s+\*\*‡∏ñ‡∏≤‡∏°:\*\*', next_line) or next_line.startswith('## ‡∏´‡∏°‡∏ß‡∏î') or next_line.startswith('### ‡∏´‡∏°‡∏ß‡∏î'):
                    break
                else:
                    j += 1
            
            answer = " ".join(answer_lines).strip()
            
            if question_text and answer:
                full_content = f"‡∏´‡∏°‡∏ß‡∏î: {current_category}"
                if current_subcategory:
                    full_content += f" / {current_subcategory}"
                full_content += f"\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question_text}\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer}"
                
                metadata = {
                    "source": "Q&A", 
                    "category": current_category
                }
                if current_subcategory:
                    metadata["subcategory"] = current_subcategory
                
                qna_docs.append(Document(page_content=full_content, metadata=metadata))
            
            i = j # Move index to after the answer
        else:
            i += 1 # Move to next line if not a question

    return qna_docs

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store"""
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡πÅ‡∏ö‡∏ö Smart Chunking...")
    
    if not os.path.exists(KB_MARKDOWN_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {KB_MARKDOWN_PATH}")
        return
    if not os.path.exists(QNA_MARKDOWN_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Q&A ‡∏ó‡∏µ‡πà: {QNA_MARKDOWN_PATH}")
        return

    all_documents = []

    # --- 1. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å knowledge_base.md ---
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å: {KB_MARKDOWN_PATH}")
    loader_kb = TextLoader(KB_MARKDOWN_PATH, encoding="utf-8")
    full_text_kb = loader_kb.load()[0].page_content
    
    # ‡πÉ‡∏ä‡πâ re.split ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô ---[SECTION:NAME]---
    section_delimiter_pattern = r'---\[SECTION:(.*?)\]---'
    parts_kb = re.split(section_delimiter_pattern, full_text_kb)

    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å re.split
    # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô [‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô‡πÅ‡∏£‡∏Å, ‡∏ä‡∏∑‡πà‡∏≠section1, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤section1, ‡∏ä‡∏∑‡πà‡∏≠section2, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤section2, ...]
    section_map_kb = {}
    if len(parts_kb) > 1:
        # ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å (index 0) ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà ‡∏ä‡∏∑‡πà‡∏≠ ‡∏Å‡∏±‡∏ö ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        section_names_kb = parts_kb[1::2]
        section_contents_kb = parts_kb[2::2]
        section_map_kb = {name.strip(): content.strip() for name, content in zip(section_names_kb, section_contents_kb)}

    if not section_map_kb:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô '---[SECTION:...]--' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå knowledge_base.md! ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Recursive ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ó‡∏ô")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        kb_documents = text_splitter.create_documents([full_text_kb])
        all_documents.extend(kb_documents)
    else:
        # --- 2. ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Chunking ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô ---
        print(f"‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ {len(section_map_kb)} ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô knowledge_base.md! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠ Section ‡πÅ‡∏•‡∏∞‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
        strategy_map = {
            "DEFINITIONS": ("üìù ...", chunk_definitions, {}),
            "RULES": ("üìú ...", chunk_by_headers, {}),
            "HOW_TO_GUIDE": ("üë£ ...", chunk_by_headers, {}),
            "MAINTENANCE": ("‚öôÔ∏è ...", chunk_by_headers, {}),
            "TIMELINES": ("‚è∞ ...", chunk_by_headers, {}),
            "PLANTING_DENSITY": ("üå≥ ...", chunk_table_like_data, {"chunk_prefix": "‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏£‡πà"}),
            "MINIMUM_AREA": ("üìè ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'MINIMUM_AREA' ‡πÅ‡∏ö‡∏ö chunk-by-header...", chunk_by_headers, {}) 
        }

        for name, content in section_map_kb.items():
            if name in strategy_map:
                message, chunk_func, kwargs = strategy_map[name]
                print(f"  - {message}")
                docs = chunk_func(content, **kwargs)
                for doc in docs:
                    doc.metadata["source"] = name.lower() # ‡πÉ‡∏™‡πà metadata ‡∏ö‡∏≠‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤
                all_documents.extend(docs)
                print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")
            else:
                print(f"  - ‚ùì ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{name}' ‡πÉ‡∏ô knowledge_base.md, ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Recursive ‡πÅ‡∏ó‡∏ô...")
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
                docs = text_splitter.create_documents([content])
                all_documents.extend(docs)

    # --- 2. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å Q&A.md ---
    print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å: {QNA_MARKDOWN_PATH}")
    loader_qna = TextLoader(QNA_MARKDOWN_PATH, encoding="utf-8")
    full_text_qna = loader_qna.load()[0].page_content
    
    qna_documents = parse_qna_markdown(full_text_qna)
    all_documents.extend(qna_documents)
    print(f"  -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(qna_documents)} Q&A chunks")


    if not all_documents:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏î‡πÜ ‡πÑ‡∏î‡πâ! ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
        return

    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ---
    print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_documents)} ‡∏ä‡∏¥‡πâ‡∏ô...")
    
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Embedding Model: {EMBEDDING_MODEL}")
    # ‡πÉ‡∏ä‡πâ GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL, 
        model_kwargs={'device': 'cuda' if 'CUDA_VISIBLE_DEVICES' in os.environ else 'cpu'}
    )
    
    if os.path.exists(VECTORSTORE_PATH):
        print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö Vector Store ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà '{VECTORSTORE_PATH}'...")
        shutil.rmtree(VECTORSTORE_PATH)

    db = FAISS.from_documents(all_documents, embeddings)
    db.save_local(VECTORSTORE_PATH)
    print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store (Smart Chunking v2) ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {VECTORSTORE_PATH}")

if __name__ == "__main__":
    main()