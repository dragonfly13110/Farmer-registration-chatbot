import os
import re
import shutil

from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_core.documents import Document

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ---
KB_DOCX_PATH = "data/knowledge_base.docx" # ‡πÑ‡∏ü‡∏•‡πå‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á
VECTORSTORE_PATH = "vectorstore_smart_chunking" # ‡∏ä‡∏∑‡πà‡∏≠ Vector Store ‡πÉ‡∏´‡∏°‡πà
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Chunking ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ ---

def chunk_by_headers(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ Markdown (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢, ‡∏Å‡∏é‡πÄ‡∏Å‡∏ì‡∏ë‡πå)
    ‡πÉ‡∏ä‡πâ #, ##, ### ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ï‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô
    """
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on,
        strip_headers=False # ‡∏Ñ‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô context
    )
    docs = markdown_splitter.split_text(text)
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ã‡∏≠‡∏¢‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    final_docs = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    for doc in docs:
        if len(doc.page_content) > 800:
            sub_docs = text_splitter.create_documents([doc.page_content], metadatas=[doc.metadata])
            final_docs.extend(sub_docs)
        else:
            final_docs.append(doc)
    return final_docs

def chunk_definitions(text: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏®‡∏±‡∏û‡∏ó‡πå (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏Ñ‡∏∑‡∏≠ 1 chunk)
    ‡πÉ‡∏ä‡πâ Regular Expression ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô "1. ‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£")
    """
    # ‡∏°‡∏≠‡∏á‡∏´‡∏≤ Pattern "#### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç. ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á" ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    pattern = re.compile(r"(####\s*\d+\..*?(?=\n####\s*\d+\.|\Z))", re.DOTALL)
    definitions = pattern.findall(text)
    
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Document
    docs = [Document(page_content=d.strip()) for d in definitions if d.strip()]
    return docs

def chunk_table_like_data(text: str, chunk_prefix: str) -> list[Document]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß/‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏∑‡∏≠ 1 chunk)
    ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£ split by newline ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏° Prefix ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ context ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    """
    # ‡∏•‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
    lines = text.strip().split('\n')
    header = lines[0] # ‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡πâ
    items = [line.strip() for line in lines[2:] if line.strip().startswith('*')] # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£

    docs = []
    for item in items:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ chunk ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏¢‡πà‡∏≠‡∏¢
        # ‡∏ô‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ * ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤‡∏≠‡∏≠‡∏Å
        clean_item = item.replace('*', '').replace('**', '').strip()
        page_content = f"{chunk_prefix}: {clean_item}"
        docs.append(Document(page_content=page_content))
    return docs

def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡πÅ‡∏ö‡∏ö Smart Chunking...")
    
    if not os.path.exists(KB_DOCX_PATH):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {KB_DOCX_PATH}")
        return

    # --- 1. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏© ---
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å: {KB_DOCX_PATH}")
    loader = Docx2txtLoader(KB_DOCX_PATH)
    full_text = loader.load()[0].page_content
    
    # ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏≤‡∏° ---[SECTION:NAME]---
    # (?s) ‡∏Ñ‡∏∑‡∏≠ re.DOTALL, (.*?) ‡∏Ñ‡∏∑‡∏≠ non-greedy match
    sections = re.findall(r"---SECTION:(.*?)---\n(?s)(.*?)(?=---SECTION:|$)", full_text)
    section_map = {name.strip(): content.strip() for name, content in sections}
    
    if not section_map:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏±‡πà‡∏ô '---[SECTION:...]--' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå! ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Recursive ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏ó‡∏ô")
        # Fallback to simple recursive splitter if no sections found
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        all_documents = text_splitter.create_documents([full_text])
    else:
        # --- 2. ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Chunking ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô ---
        print("‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ {len(section_map)} ‡∏™‡πà‡∏ß‡∏ô! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô...")
        all_documents = []

        if "DEFINITIONS" in section_map:
            print("  - üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'DEFINITIONS' ‡πÅ‡∏ö‡∏ö chunk-per-definition...")
            docs = chunk_definitions(section_map["DEFINITIONS"])
            for doc in docs: doc.metadata["source"] = "definitions"
            all_documents.extend(docs)
            print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")

        if "RULES" in section_map:
            print("  - üìú ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'RULES' ‡πÅ‡∏ö‡∏ö chunk-by-header...")
            docs = chunk_by_headers(section_map["RULES"])
            for doc in docs: doc.metadata["source"] = "rules_and_conditions"
            all_documents.extend(docs)
            print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")
            
        if "HOW_TO_GUIDE" in section_map:
            print("  - üë£ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'HOW_TO_GUIDE' ‡πÅ‡∏ö‡∏ö chunk-by-header...")
            docs = chunk_by_headers(section_map["HOW_TO_GUIDE"])
            for doc in docs: doc.metadata["source"] = "how_to_guide"
            all_documents.extend(docs)
            print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")
            
        if "TIMELINES" in section_map:
            print("  - ‚è∞ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'TIMELINES' ‡πÅ‡∏ö‡∏ö chunk-by-header...")
            docs = chunk_by_headers(section_map["TIMELINES"])
            for doc in docs: doc.metadata["source"] = "timelines"
            all_documents.extend(docs)
            print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")

        if "PLANTING_DENSITY" in section_map:
            print("  - üå≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 'PLANTING_DENSITY' ‡πÅ‡∏ö‡∏ö chunk-per-item...")
            docs = chunk_table_like_data(
                section_map["PLANTING_DENSITY"],
                chunk_prefix="‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏£‡πà"
            )
            for doc in docs: doc.metadata["source"] = "planting_density"
            all_documents.extend(docs)
            print(f"    -> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {len(docs)} chunks")

    if not all_documents:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏î‡πÜ ‡πÑ‡∏î‡πâ! ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
        return

    # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ---
    print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_documents)} ‡∏ä‡∏¥‡πâ‡∏ô...")
    
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Embedding Model: {EMBEDDING_MODEL}")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={'device': 'cuda'}) # Use 'cpu' if no GPU
    
    if os.path.exists(VECTORSTORE_PATH):
        print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö Vector Store ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà '{VECTORSTORE_PATH}'...")
        shutil.rmtree(VECTORSTORE_PATH)

    db = FAISS.from_documents(all_documents, embeddings)
    db.save_local(VECTORSTORE_PATH)
    print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡πÅ‡∏ö‡∏ö Smart Chunking ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: {VECTORSTORE_PATH}")


if __name__ == "__main__":
    main()